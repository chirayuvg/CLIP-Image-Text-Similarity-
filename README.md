# CLIP-Image-Text-Similarity
This project implements an image-text similarity matching system using OpenAI's CLIP (Contrastive Language-Image Pre-Training) model. It processes images and text from the COCO dataset, computes their semantic similarities, and visualizes the results.

# Features
  * Image and text feature extraction using CLIP
  * Semantic similarity matching between images and text
  * Batch processing with GPU acceleration
  * Interactive visualization of similarity scores
  * Support for top-k matching results
  * COCO dataset integration

# Prerequisites
  * Python 3.7+
  * CUDA-capable GPU (recommended)
  * Internet connection for dataset download
